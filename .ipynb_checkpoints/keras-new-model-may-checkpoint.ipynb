{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from onısleme import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LSTM,\\\n",
    "    BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/denizekiz1991/uncategorized/runs/sla6u2d0\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vectors of label - 'sad': 100%|██████████| 314/314 [00:07<00:00, 44.66it/s]\n",
      "Saving vectors of label - 'happy': 100%|██████████| 323/323 [00:07<00:00, 42.21it/s]\n",
      "Saving vectors of label - 'fearful': 100%|██████████| 321/321 [00:06<00:00, 47.13it/s]\n",
      "Saving vectors of label - 'angry': 100%|██████████| 300/300 [00:06<00:00, 49.42it/s]\n"
     ]
    }
   ],
   "source": [
    "wandb.init()\n",
    "config = wandb.config\n",
    "\n",
    "config.max_len = 11\n",
    "config.buckets = 20\n",
    "\n",
    "# Save data to array file first\n",
    "save_data_to_array(max_len=config.max_len, n_mfcc=config.buckets)\n",
    "\n",
    "#labels=[\"Abaküs\",\"Ağaçkakan\",\"Akademisyen\",\"Akaryakıt\",\"Akışkan\",\"Akrobat\",\"Akvaryum\",\"Albatros\",\"Ameliyat\",\"Asayiş\",\"Bağışıklık\",\"Basmakalıp\",\"Bilimkurgu\",\"Bölgesel\",\"Bukalemun\",\"Çağrışım\",\"Camgöbeği\",\"Cankurtaran\",\"Cengaver\",\"Çepeçevre\",\"Cereyan\",\"Cihangir\",\"Çimento\",\"Çıngırak\",\"Coğrafya\",\"Çuvaldız\",\"Dağarcık\",\"Dahiliye\",\"Davlumbaz\",\"Değirmen\",\"Deniz\",\"Dışbükey\",\"Dolambaç\",\"Dönence\",\"Dramatik\",\"Ejderha\",\"Elmas\",\"Encümen\",\"Erişim\",\"Esrarengiz\",\"Etkileşim\",\"Ettirgen\",\"Faaliyet\",\"Figüran\",\"Filozof\",\"Fiyasko\",\"Fizyoterapi\",\"Fonksiyon\",\"Fragman\",\"Geleneksel\",\"Gelişigüzel\",\"Gergedan\",\"Geribildirim\",\"Giderayak\",\"Giriş\",\"Gümbürtü\",\"Hapşırık\",\"Harmandalı\",\"Hassasiyet\",\"Hava\",\"Hayalperest\",\"Hayırsever\",\"Hazırcevap\",\"Heykeltıraş\",\"Hokkabaz\",\"Ihlamur\",\"İlkokul\",\"İmtiyaz\",\"İndüksiyon\",\"İnorganik\",\"Iraksak\",\"Isırgan\",\"İskelet\",\"Ispanak\",\"Istakoz\",\"İzafiyet\",\"Jelatin\",\"Jenerasyon\",\"Jeneratör\",\"Jeotermal\",\"Jimnastik\",\"Jüpiter\",\"Kaç\",\"Kadastro\",\"Kahkaha\",\"Kalemtıraş\",\"Kamyonet\",\"Kaplumbağa\",\"Karnabahar\",\"Katarakt\",\"Kaynakça\",\"Kazandibi\",\"Kilitle\",\"Kırlangıç\",\"Komposto\",\"Külbastı\",\"Labirent\",\"Lacivert\",\"Lahmacun\",\"Leblebi\",\"Limonata\",\"Lojistik\",\"Lokomotif\",\"Madalyon\",\"Mancınık\",\"Mandalina\",\"Manyetik\",\"Margarin\",\"Menemen\",\"Merhaba\",\"Mıknatıs\",\"Muhallebi\",\"Mürekkep\",\"Muşamba\",\"Müşterek\",\"Müzisyen\",\"Naftalin\",\"Nakliyat\",\"Nasıl\",\"Nedir\",\"Nevresim\",\"Nikola\",\"Nostalji\",\"Numune\",\"Objektif\",\"Oksijen\",\"Öksürük\",\"Okyanus\",\"Öncelik\",\"Orkestra\",\"Otantik\",\"Oyuncak\",\"Özeleştiri\",\"Özgeçmiş\",\"Özgüven\",\"Palyaço\",\"Panayır\",\"Pansiyon\",\"Paraşüt\",\"Patates\",\"Penguen\",\"Piramit\",\"Porselen\",\"Raptiye\",\"Rastlantı\",\"Rengarenk\",\"Rivayet\",\"Rutubet\",\"Saat\",\"Saklambaç\",\"Salıncak\",\"Salyangoz\",\"Serüven\",\"Serzeniş\",\"Silahşör\",\"Süpürge\",\"Tahammül\",\"Tarih\",\"Tekerleme\",\"Teneffüs\",\"Tepegöz\",\"Tercüman\",\"Tertibat\",\"Tolerans\",\"Tulumba\",\"Tümevarım\",\"Uçurtma\",\"Ulaşım\",\"Üniforma\",\"Üniversite\",\"Üretken\",\"Uskumru\",\"Üstgeçit\",\"Üstünkörü\",\"Uygarlık\",\"Uyurgezer\",\"Vanilya\",\"Vantilatör\",\"Varsayım\",\"Veteriner\",\"Veznedar\",\"Vitamin\",\"Volkanik\",\"Yakamoz\",\"Yanardağ\",\"Yelkovan\",\"Yetenek\",\"Yönetmen\",\"Yumurta\",\"Yüzeysel\",\"Zanaat\",\"Zemheri\",\"Zımpara\",\"Zürafa\" ]\n",
    "label=[\"angry\",\"fearful\",\"happy\",\"sad\"]\n",
    "#label=[\"bed\",\"bird\",\"cat\",\"dog\",\"down\",\"eight\",\"five\",\"happy\",\"house\",\"zero\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature dimension\n",
    "channels = 1\n",
    "config.epochs = 300\n",
    "config.batch_size = 128\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], config.buckets, config.max_len, channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], config.buckets, config.max_len, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f135b97dc50>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[820, :, :, 0])\n",
    "print(y_train[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deniz/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], config.buckets, config.max_len)\n",
    "X_test = X_test.reshape(X_test.shape[0], config.buckets, config.max_len)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(config.buckets, config.max_len)))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/denizekiz1991/uncategorized/runs/aabq45bj\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.1 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deniz/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 880 samples, validate on 378 samples\n",
      "Epoch 1/300\n",
      "880/880 [==============================] - 2s 2ms/step - loss: 12.1097 - acc: 0.2466 - val_loss: 12.0424 - val_acc: 0.2487\n",
      "Epoch 2/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0537 - acc: 0.2511 - val_loss: 12.0042 - val_acc: 0.2513\n",
      "Epoch 3/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0116 - val_acc: 0.2513\n",
      "Epoch 4/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 5/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 6/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 7/300\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 8/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 9/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 10/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 11/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 12/300\n",
      "880/880 [==============================] - 0s 44us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 13/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 14/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 15/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 16/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 17/300\n",
      "880/880 [==============================] - 0s 41us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 18/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 19/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 20/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 21/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 22/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 23/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 24/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 25/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 26/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 27/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 28/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 29/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 30/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 31/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 32/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 33/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 34/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 35/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 36/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 37/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 38/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 39/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 40/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 41/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 42/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 43/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 44/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 45/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 46/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 47/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 48/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 49/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 50/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 51/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 52/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 53/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 54/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 55/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 56/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 57/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 59/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 60/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 61/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 62/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 63/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 64/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 65/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 66/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 67/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 68/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 69/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 70/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 71/300\n",
      "880/880 [==============================] - 0s 37us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 72/300\n",
      "880/880 [==============================] - 0s 40us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 73/300\n",
      "880/880 [==============================] - 0s 40us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 74/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 75/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 76/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 77/300\n",
      "880/880 [==============================] - 0s 39us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 78/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 79/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 80/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 81/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 82/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 83/300\n",
      "880/880 [==============================] - 0s 37us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 84/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 85/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 86/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 87/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 88/300\n",
      "880/880 [==============================] - 0s 56us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 89/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 90/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 91/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 92/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 93/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 94/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 95/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 96/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 97/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 98/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 99/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 100/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 101/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 102/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 103/300\n",
      "880/880 [==============================] - 0s 39us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 104/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 105/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 106/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 107/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 108/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 109/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 110/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 111/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 112/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 113/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 114/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 115/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 116/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 117/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 119/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 120/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 121/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 122/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 123/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 124/300\n",
      "880/880 [==============================] - 0s 41us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 125/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 126/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 127/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 128/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 129/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 130/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 131/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 132/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 133/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 134/300\n",
      "880/880 [==============================] - 0s 37us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 135/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 136/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 137/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 138/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 139/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 140/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 141/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 142/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 143/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 144/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 145/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 146/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 147/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 148/300\n",
      "880/880 [==============================] - 0s 43us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 149/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 150/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 151/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 152/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 153/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 154/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 155/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 156/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 157/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 158/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 159/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 160/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 161/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 162/300\n",
      "880/880 [==============================] - 0s 39us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 163/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 164/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 165/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 166/300\n",
      "880/880 [==============================] - 0s 37us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 167/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 168/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 169/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 170/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 171/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 172/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 173/300\n",
      "880/880 [==============================] - 0s 39us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 174/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 175/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 176/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 177/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 178/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 179/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 180/300\n",
      "880/880 [==============================] - 0s 39us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 181/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 182/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 183/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 184/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 185/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 186/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 187/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 188/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 189/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 190/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 191/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 192/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 193/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 194/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 195/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 196/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 197/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 198/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 199/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 200/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 201/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 202/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 203/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 204/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 205/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 206/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 207/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 208/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 209/300\n",
      "880/880 [==============================] - 0s 46us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 210/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 211/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 212/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 213/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 214/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 215/300\n",
      "880/880 [==============================] - 0s 45us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 216/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 217/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 218/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 219/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 220/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 221/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 222/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 223/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 224/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 225/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 226/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 227/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 228/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 229/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 230/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 231/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 232/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 233/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 234/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 235/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 237/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 238/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 239/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 240/300\n",
      "880/880 [==============================] - 0s 37us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 241/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 242/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 243/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 244/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 245/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 246/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 247/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 248/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 249/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 250/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 251/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 252/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 253/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 254/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 255/300\n",
      "880/880 [==============================] - 0s 37us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 256/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 257/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 258/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 259/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 260/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 261/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 262/300\n",
      "880/880 [==============================] - 0s 43us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 263/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 264/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 265/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 266/300\n",
      "880/880 [==============================] - 0s 34us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 267/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 268/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 269/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 270/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 271/300\n",
      "880/880 [==============================] - 0s 37us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 272/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 273/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 274/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 275/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 276/300\n",
      "880/880 [==============================] - 0s 30us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 277/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 278/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 279/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 280/300\n",
      "880/880 [==============================] - 0s 41us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 281/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 282/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 283/300\n",
      "880/880 [==============================] - 0s 29us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 284/300\n",
      "880/880 [==============================] - 0s 28us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 285/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 286/300\n",
      "880/880 [==============================] - 0s 31us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 287/300\n",
      "880/880 [==============================] - 0s 35us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 288/300\n",
      "880/880 [==============================] - 0s 44us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 289/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 290/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 291/300\n",
      "880/880 [==============================] - 0s 36us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 292/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 293/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 294/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 296/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 297/300\n",
      "880/880 [==============================] - 0s 38us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 298/300\n",
      "880/880 [==============================] - 0s 32us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 299/300\n",
      "880/880 [==============================] - 0s 27us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n",
      "Epoch 300/300\n",
      "880/880 [==============================] - 0s 33us/step - loss: 12.0519 - acc: 0.2523 - val_loss: 12.0121 - val_acc: 0.2513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1356a49e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "model.fit(X_train, y_train_hot, epochs=config.epochs, validation_data=(X_test, y_test_hot), callbacks=[WandbCallback(data_type=\"image\", labels=label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "X_train = X_train.reshape(X_train.shape[0], config.buckets, config.max_len)\n",
    "X_test = X_test.reshape(X_test.shape[0], config.buckets, config.max_len)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(config.buckets, config.max_len), activation=\"sigmoid\"))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='tanh'))\n",
    "\n",
    "#model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/denizekiz1991/uncategorized/runs/jpz5j7dq\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "Train on 880 samples, validate on 378 samples\n",
      "Epoch 1/300\n",
      "880/880 [==============================] - 1s 1ms/step - loss: 0.5722 - acc: 0.7483 - val_loss: 0.5678 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.1 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "880/880 [==============================] - 0s 447us/step - loss: 0.5614 - acc: 0.7500 - val_loss: 0.5626 - val_acc: 0.7500\n",
      "Epoch 3/300\n",
      "880/880 [==============================] - 0s 441us/step - loss: 0.5594 - acc: 0.7497 - val_loss: 0.5621 - val_acc: 0.7500\n",
      "Epoch 4/300\n",
      "880/880 [==============================] - 0s 446us/step - loss: 0.5581 - acc: 0.7503 - val_loss: 0.5598 - val_acc: 0.7500\n",
      "Epoch 5/300\n",
      "880/880 [==============================] - 0s 459us/step - loss: 0.5559 - acc: 0.7514 - val_loss: 0.5584 - val_acc: 0.7507\n",
      "Epoch 6/300\n",
      "880/880 [==============================] - 0s 466us/step - loss: 0.5512 - acc: 0.7506 - val_loss: 0.5582 - val_acc: 0.7513\n",
      "Epoch 7/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.5506 - acc: 0.7514 - val_loss: 0.5566 - val_acc: 0.7513\n",
      "Epoch 8/300\n",
      "880/880 [==============================] - 0s 455us/step - loss: 0.5499 - acc: 0.7526 - val_loss: 0.5569 - val_acc: 0.7513\n",
      "Epoch 9/300\n",
      "880/880 [==============================] - 0s 454us/step - loss: 0.5479 - acc: 0.7528 - val_loss: 0.5558 - val_acc: 0.7513\n",
      "Epoch 10/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.5439 - acc: 0.7540 - val_loss: 0.5569 - val_acc: 0.7507\n",
      "Epoch 11/300\n",
      "880/880 [==============================] - 0s 461us/step - loss: 0.5475 - acc: 0.7531 - val_loss: 0.5564 - val_acc: 0.7507\n",
      "Epoch 12/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.5437 - acc: 0.7554 - val_loss: 0.5576 - val_acc: 0.7507\n",
      "Epoch 13/300\n",
      "880/880 [==============================] - 0s 462us/step - loss: 0.5442 - acc: 0.7531 - val_loss: 0.5576 - val_acc: 0.7480\n",
      "Epoch 14/300\n",
      "880/880 [==============================] - 0s 452us/step - loss: 0.5420 - acc: 0.7568 - val_loss: 0.5582 - val_acc: 0.7487\n",
      "Epoch 15/300\n",
      "880/880 [==============================] - 0s 472us/step - loss: 0.5378 - acc: 0.7574 - val_loss: 0.5577 - val_acc: 0.7467\n",
      "Epoch 16/300\n",
      "880/880 [==============================] - 0s 450us/step - loss: 0.5339 - acc: 0.7568 - val_loss: 0.5586 - val_acc: 0.7467\n",
      "Epoch 17/300\n",
      "880/880 [==============================] - 0s 484us/step - loss: 0.5371 - acc: 0.7565 - val_loss: 0.5571 - val_acc: 0.7487\n",
      "Epoch 18/300\n",
      "880/880 [==============================] - 0s 455us/step - loss: 0.5373 - acc: 0.7585 - val_loss: 0.5586 - val_acc: 0.7480\n",
      "Epoch 19/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.5362 - acc: 0.7574 - val_loss: 0.5559 - val_acc: 0.7474\n",
      "Epoch 20/300\n",
      "880/880 [==============================] - 0s 456us/step - loss: 0.5349 - acc: 0.7577 - val_loss: 0.5593 - val_acc: 0.7474\n",
      "Epoch 21/300\n",
      "880/880 [==============================] - 0s 476us/step - loss: 0.5348 - acc: 0.7582 - val_loss: 0.5572 - val_acc: 0.7467\n",
      "Epoch 22/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.5313 - acc: 0.7599 - val_loss: 0.5563 - val_acc: 0.7460\n",
      "Epoch 23/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.5298 - acc: 0.7591 - val_loss: 0.5592 - val_acc: 0.7467\n",
      "Epoch 24/300\n",
      "880/880 [==============================] - 0s 461us/step - loss: 0.5291 - acc: 0.7599 - val_loss: 0.5581 - val_acc: 0.7460\n",
      "Epoch 25/300\n",
      "880/880 [==============================] - 0s 454us/step - loss: 0.5255 - acc: 0.7628 - val_loss: 0.5581 - val_acc: 0.7487\n",
      "Epoch 26/300\n",
      "880/880 [==============================] - 0s 446us/step - loss: 0.5273 - acc: 0.7594 - val_loss: 0.5580 - val_acc: 0.7480\n",
      "Epoch 27/300\n",
      "880/880 [==============================] - 0s 447us/step - loss: 0.5222 - acc: 0.7619 - val_loss: 0.5606 - val_acc: 0.7480\n",
      "Epoch 28/300\n",
      "880/880 [==============================] - 0s 458us/step - loss: 0.5235 - acc: 0.7636 - val_loss: 0.5618 - val_acc: 0.7474\n",
      "Epoch 29/300\n",
      "880/880 [==============================] - 0s 459us/step - loss: 0.5210 - acc: 0.7631 - val_loss: 0.5635 - val_acc: 0.7474\n",
      "Epoch 30/300\n",
      "880/880 [==============================] - 0s 463us/step - loss: 0.5218 - acc: 0.7616 - val_loss: 0.5616 - val_acc: 0.7487\n",
      "Epoch 31/300\n",
      "880/880 [==============================] - 0s 464us/step - loss: 0.5172 - acc: 0.7636 - val_loss: 0.5619 - val_acc: 0.7460\n",
      "Epoch 32/300\n",
      "880/880 [==============================] - 0s 462us/step - loss: 0.5149 - acc: 0.7670 - val_loss: 0.5640 - val_acc: 0.7493\n",
      "Epoch 33/300\n",
      "880/880 [==============================] - 0s 538us/step - loss: 0.5166 - acc: 0.7665 - val_loss: 0.5595 - val_acc: 0.7474\n",
      "Epoch 34/300\n",
      "880/880 [==============================] - 1s 590us/step - loss: 0.5155 - acc: 0.7631 - val_loss: 0.5627 - val_acc: 0.7487\n",
      "Epoch 35/300\n",
      "880/880 [==============================] - 1s 589us/step - loss: 0.5137 - acc: 0.7682 - val_loss: 0.5641 - val_acc: 0.7467\n",
      "Epoch 36/300\n",
      "880/880 [==============================] - 1s 595us/step - loss: 0.5114 - acc: 0.7662 - val_loss: 0.5629 - val_acc: 0.7480\n",
      "Epoch 37/300\n",
      "880/880 [==============================] - 1s 591us/step - loss: 0.5136 - acc: 0.7639 - val_loss: 0.5634 - val_acc: 0.7500\n",
      "Epoch 38/300\n",
      "880/880 [==============================] - 1s 578us/step - loss: 0.5066 - acc: 0.7713 - val_loss: 0.5631 - val_acc: 0.7454\n",
      "Epoch 39/300\n",
      "880/880 [==============================] - 1s 589us/step - loss: 0.5085 - acc: 0.7696 - val_loss: 0.5618 - val_acc: 0.7434\n",
      "Epoch 40/300\n",
      "880/880 [==============================] - 0s 471us/step - loss: 0.5087 - acc: 0.7685 - val_loss: 0.5636 - val_acc: 0.7454\n",
      "Epoch 41/300\n",
      "880/880 [==============================] - 0s 467us/step - loss: 0.5007 - acc: 0.7753 - val_loss: 0.5640 - val_acc: 0.7493\n",
      "Epoch 42/300\n",
      "880/880 [==============================] - 0s 469us/step - loss: 0.5023 - acc: 0.7744 - val_loss: 0.5628 - val_acc: 0.7480\n",
      "Epoch 43/300\n",
      "880/880 [==============================] - 0s 463us/step - loss: 0.5007 - acc: 0.7730 - val_loss: 0.5625 - val_acc: 0.7460\n",
      "Epoch 44/300\n",
      "880/880 [==============================] - 0s 464us/step - loss: 0.5008 - acc: 0.7724 - val_loss: 0.5653 - val_acc: 0.7407\n",
      "Epoch 45/300\n",
      "880/880 [==============================] - 0s 467us/step - loss: 0.4963 - acc: 0.7767 - val_loss: 0.5649 - val_acc: 0.7434\n",
      "Epoch 46/300\n",
      "880/880 [==============================] - 0s 465us/step - loss: 0.4940 - acc: 0.7798 - val_loss: 0.5651 - val_acc: 0.7434\n",
      "Epoch 47/300\n",
      "880/880 [==============================] - 0s 462us/step - loss: 0.4931 - acc: 0.7773 - val_loss: 0.5690 - val_acc: 0.7421\n",
      "Epoch 48/300\n",
      "880/880 [==============================] - 0s 467us/step - loss: 0.4887 - acc: 0.7767 - val_loss: 0.5659 - val_acc: 0.7480\n",
      "Epoch 49/300\n",
      "880/880 [==============================] - 0s 485us/step - loss: 0.4857 - acc: 0.7778 - val_loss: 0.5664 - val_acc: 0.7407\n",
      "Epoch 50/300\n",
      "880/880 [==============================] - 0s 464us/step - loss: 0.4816 - acc: 0.7852 - val_loss: 0.5702 - val_acc: 0.7447\n",
      "Epoch 51/300\n",
      "880/880 [==============================] - 0s 473us/step - loss: 0.4866 - acc: 0.7773 - val_loss: 0.5714 - val_acc: 0.7394\n",
      "Epoch 52/300\n",
      "880/880 [==============================] - 0s 480us/step - loss: 0.4867 - acc: 0.7821 - val_loss: 0.5695 - val_acc: 0.7414\n",
      "Epoch 53/300\n",
      "880/880 [==============================] - 0s 468us/step - loss: 0.4869 - acc: 0.7759 - val_loss: 0.5706 - val_acc: 0.7434\n",
      "Epoch 54/300\n",
      "880/880 [==============================] - 0s 477us/step - loss: 0.4817 - acc: 0.7818 - val_loss: 0.5690 - val_acc: 0.7407\n",
      "Epoch 55/300\n",
      "880/880 [==============================] - 0s 479us/step - loss: 0.4807 - acc: 0.7858 - val_loss: 0.5719 - val_acc: 0.7447\n",
      "Epoch 56/300\n",
      "880/880 [==============================] - 0s 474us/step - loss: 0.4712 - acc: 0.7892 - val_loss: 0.5668 - val_acc: 0.7421\n",
      "Epoch 57/300\n",
      "880/880 [==============================] - 0s 480us/step - loss: 0.4757 - acc: 0.7864 - val_loss: 0.5726 - val_acc: 0.7407\n",
      "Epoch 58/300\n",
      "880/880 [==============================] - 0s 467us/step - loss: 0.4718 - acc: 0.7881 - val_loss: 0.5722 - val_acc: 0.7467\n",
      "Epoch 59/300\n",
      "880/880 [==============================] - 0s 473us/step - loss: 0.4724 - acc: 0.7898 - val_loss: 0.5761 - val_acc: 0.7447\n",
      "Epoch 60/300\n",
      "880/880 [==============================] - 0s 471us/step - loss: 0.4635 - acc: 0.7938 - val_loss: 0.5756 - val_acc: 0.7440\n",
      "Epoch 61/300\n",
      "880/880 [==============================] - 0s 473us/step - loss: 0.4626 - acc: 0.7946 - val_loss: 0.5797 - val_acc: 0.7407\n",
      "Epoch 62/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 455us/step - loss: 0.4629 - acc: 0.7929 - val_loss: 0.5749 - val_acc: 0.7447\n",
      "Epoch 63/300\n",
      "880/880 [==============================] - 0s 441us/step - loss: 0.4603 - acc: 0.7932 - val_loss: 0.5772 - val_acc: 0.7414\n",
      "Epoch 64/300\n",
      "880/880 [==============================] - 0s 432us/step - loss: 0.4633 - acc: 0.7938 - val_loss: 0.5761 - val_acc: 0.7454\n",
      "Epoch 65/300\n",
      "880/880 [==============================] - 0s 455us/step - loss: 0.4631 - acc: 0.7881 - val_loss: 0.5851 - val_acc: 0.7407\n",
      "Epoch 66/300\n",
      "880/880 [==============================] - 0s 453us/step - loss: 0.4526 - acc: 0.7960 - val_loss: 0.5777 - val_acc: 0.7480\n",
      "Epoch 67/300\n",
      "880/880 [==============================] - 0s 455us/step - loss: 0.4627 - acc: 0.7889 - val_loss: 0.5786 - val_acc: 0.7427\n",
      "Epoch 68/300\n",
      "880/880 [==============================] - 0s 440us/step - loss: 0.4609 - acc: 0.7932 - val_loss: 0.5815 - val_acc: 0.7388\n",
      "Epoch 69/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.4550 - acc: 0.7901 - val_loss: 0.5801 - val_acc: 0.7427\n",
      "Epoch 70/300\n",
      "880/880 [==============================] - 0s 464us/step - loss: 0.4570 - acc: 0.7932 - val_loss: 0.5723 - val_acc: 0.7388\n",
      "Epoch 71/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.4538 - acc: 0.7943 - val_loss: 0.5793 - val_acc: 0.7454\n",
      "Epoch 72/300\n",
      "880/880 [==============================] - 0s 437us/step - loss: 0.4480 - acc: 0.7989 - val_loss: 0.5899 - val_acc: 0.7401\n",
      "Epoch 73/300\n",
      "880/880 [==============================] - 0s 450us/step - loss: 0.4580 - acc: 0.7918 - val_loss: 0.5945 - val_acc: 0.7421\n",
      "Epoch 74/300\n",
      "880/880 [==============================] - 0s 450us/step - loss: 0.4404 - acc: 0.7994 - val_loss: 0.6015 - val_acc: 0.7388\n",
      "Epoch 75/300\n",
      "880/880 [==============================] - 0s 456us/step - loss: 0.4455 - acc: 0.8000 - val_loss: 0.6032 - val_acc: 0.7434\n",
      "Epoch 76/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.4464 - acc: 0.7974 - val_loss: 0.5996 - val_acc: 0.7394\n",
      "Epoch 77/300\n",
      "880/880 [==============================] - 0s 463us/step - loss: 0.4428 - acc: 0.8006 - val_loss: 0.5968 - val_acc: 0.7434\n",
      "Epoch 78/300\n",
      "880/880 [==============================] - 0s 449us/step - loss: 0.4358 - acc: 0.8054 - val_loss: 0.5926 - val_acc: 0.7407\n",
      "Epoch 79/300\n",
      "880/880 [==============================] - 0s 458us/step - loss: 0.4352 - acc: 0.8048 - val_loss: 0.6078 - val_acc: 0.7414\n",
      "Epoch 80/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.4385 - acc: 0.7994 - val_loss: 0.5941 - val_acc: 0.7434\n",
      "Epoch 81/300\n",
      "880/880 [==============================] - 0s 446us/step - loss: 0.4340 - acc: 0.8045 - val_loss: 0.6006 - val_acc: 0.7434\n",
      "Epoch 82/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.4390 - acc: 0.8074 - val_loss: 0.5958 - val_acc: 0.7440\n",
      "Epoch 83/300\n",
      "880/880 [==============================] - 0s 454us/step - loss: 0.4381 - acc: 0.8003 - val_loss: 0.5927 - val_acc: 0.7394\n",
      "Epoch 84/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.4306 - acc: 0.8099 - val_loss: 0.5952 - val_acc: 0.7434\n",
      "Epoch 85/300\n",
      "880/880 [==============================] - 0s 454us/step - loss: 0.4334 - acc: 0.8077 - val_loss: 0.5961 - val_acc: 0.7427\n",
      "Epoch 86/300\n",
      "880/880 [==============================] - 0s 458us/step - loss: 0.4194 - acc: 0.8116 - val_loss: 0.6140 - val_acc: 0.7401\n",
      "Epoch 87/300\n",
      "880/880 [==============================] - 0s 455us/step - loss: 0.4273 - acc: 0.8068 - val_loss: 0.6056 - val_acc: 0.7421\n",
      "Epoch 88/300\n",
      "880/880 [==============================] - 0s 542us/step - loss: 0.4175 - acc: 0.8099 - val_loss: 0.6164 - val_acc: 0.7427\n",
      "Epoch 89/300\n",
      "880/880 [==============================] - 0s 522us/step - loss: 0.4155 - acc: 0.8131 - val_loss: 0.6159 - val_acc: 0.7467\n",
      "Epoch 90/300\n",
      "880/880 [==============================] - 0s 549us/step - loss: 0.4236 - acc: 0.8080 - val_loss: 0.6643 - val_acc: 0.7348\n",
      "Epoch 91/300\n",
      "880/880 [==============================] - 0s 483us/step - loss: 0.4293 - acc: 0.8111 - val_loss: 0.6134 - val_acc: 0.7414\n",
      "Epoch 92/300\n",
      "880/880 [==============================] - 0s 527us/step - loss: 0.4227 - acc: 0.8102 - val_loss: 0.6062 - val_acc: 0.7388\n",
      "Epoch 93/300\n",
      "880/880 [==============================] - 0s 485us/step - loss: 0.4098 - acc: 0.8187 - val_loss: 0.6208 - val_acc: 0.7388\n",
      "Epoch 94/300\n",
      "880/880 [==============================] - 0s 475us/step - loss: 0.4164 - acc: 0.8142 - val_loss: 0.6340 - val_acc: 0.7361\n",
      "Epoch 95/300\n",
      "880/880 [==============================] - 0s 485us/step - loss: 0.4214 - acc: 0.8054 - val_loss: 0.6170 - val_acc: 0.7401\n",
      "Epoch 96/300\n",
      "880/880 [==============================] - 0s 554us/step - loss: 0.4096 - acc: 0.8190 - val_loss: 0.6565 - val_acc: 0.7302\n",
      "Epoch 97/300\n",
      "880/880 [==============================] - 1s 602us/step - loss: 0.4202 - acc: 0.8153 - val_loss: 0.6225 - val_acc: 0.7374\n",
      "Epoch 98/300\n",
      "880/880 [==============================] - 1s 585us/step - loss: 0.4180 - acc: 0.8119 - val_loss: 0.6249 - val_acc: 0.7381\n",
      "Epoch 99/300\n",
      "880/880 [==============================] - 1s 660us/step - loss: 0.4025 - acc: 0.8170 - val_loss: 0.6219 - val_acc: 0.7374\n",
      "Epoch 100/300\n",
      "880/880 [==============================] - 1s 621us/step - loss: 0.4062 - acc: 0.8205 - val_loss: 0.6364 - val_acc: 0.7414\n",
      "Epoch 101/300\n",
      "880/880 [==============================] - 1s 604us/step - loss: 0.4067 - acc: 0.8176 - val_loss: 0.6258 - val_acc: 0.7474\n",
      "Epoch 102/300\n",
      "880/880 [==============================] - 1s 597us/step - loss: 0.4091 - acc: 0.8136 - val_loss: 0.6100 - val_acc: 0.7460\n",
      "Epoch 103/300\n",
      "880/880 [==============================] - 0s 465us/step - loss: 0.4011 - acc: 0.8224 - val_loss: 0.6368 - val_acc: 0.7440\n",
      "Epoch 104/300\n",
      "880/880 [==============================] - 0s 471us/step - loss: 0.4031 - acc: 0.8170 - val_loss: 0.6222 - val_acc: 0.7394\n",
      "Epoch 105/300\n",
      "880/880 [==============================] - 0s 521us/step - loss: 0.4049 - acc: 0.8187 - val_loss: 0.6322 - val_acc: 0.7427\n",
      "Epoch 106/300\n",
      "880/880 [==============================] - 0s 483us/step - loss: 0.4071 - acc: 0.8136 - val_loss: 0.6337 - val_acc: 0.7394\n",
      "Epoch 107/300\n",
      "880/880 [==============================] - 0s 474us/step - loss: 0.4118 - acc: 0.8173 - val_loss: 0.6496 - val_acc: 0.7427\n",
      "Epoch 108/300\n",
      "880/880 [==============================] - 0s 516us/step - loss: 0.4010 - acc: 0.8173 - val_loss: 0.6594 - val_acc: 0.7440\n",
      "Epoch 109/300\n",
      "880/880 [==============================] - 0s 534us/step - loss: 0.4108 - acc: 0.8119 - val_loss: 0.6349 - val_acc: 0.7348\n",
      "Epoch 110/300\n",
      "880/880 [==============================] - 0s 479us/step - loss: 0.4050 - acc: 0.8193 - val_loss: 0.6280 - val_acc: 0.7374\n",
      "Epoch 111/300\n",
      "880/880 [==============================] - 0s 500us/step - loss: 0.4038 - acc: 0.8199 - val_loss: 0.6273 - val_acc: 0.7421\n",
      "Epoch 112/300\n",
      "880/880 [==============================] - 0s 493us/step - loss: 0.4068 - acc: 0.8196 - val_loss: 0.6239 - val_acc: 0.7421\n",
      "Epoch 113/300\n",
      "880/880 [==============================] - 0s 481us/step - loss: 0.3951 - acc: 0.8210 - val_loss: 0.6644 - val_acc: 0.7394\n",
      "Epoch 114/300\n",
      "880/880 [==============================] - 0s 477us/step - loss: 0.3865 - acc: 0.8267 - val_loss: 0.6833 - val_acc: 0.7348\n",
      "Epoch 115/300\n",
      "880/880 [==============================] - 0s 480us/step - loss: 0.3946 - acc: 0.8207 - val_loss: 0.6354 - val_acc: 0.7381\n",
      "Epoch 116/300\n",
      "880/880 [==============================] - 0s 481us/step - loss: 0.3894 - acc: 0.8256 - val_loss: 0.6578 - val_acc: 0.7354\n",
      "Epoch 117/300\n",
      "880/880 [==============================] - 0s 484us/step - loss: 0.3827 - acc: 0.8327 - val_loss: 0.6620 - val_acc: 0.7427\n",
      "Epoch 118/300\n",
      "880/880 [==============================] - 0s 490us/step - loss: 0.3781 - acc: 0.8352 - val_loss: 0.6472 - val_acc: 0.7361\n",
      "Epoch 119/300\n",
      "880/880 [==============================] - 0s 484us/step - loss: 0.3844 - acc: 0.8276 - val_loss: 0.6589 - val_acc: 0.7315\n",
      "Epoch 120/300\n",
      "880/880 [==============================] - 0s 485us/step - loss: 0.3762 - acc: 0.8361 - val_loss: 0.6883 - val_acc: 0.7354\n",
      "Epoch 121/300\n",
      "880/880 [==============================] - 0s 487us/step - loss: 0.3756 - acc: 0.8327 - val_loss: 0.6528 - val_acc: 0.7354\n",
      "Epoch 122/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 470us/step - loss: 0.3825 - acc: 0.8281 - val_loss: 0.6886 - val_acc: 0.7328\n",
      "Epoch 123/300\n",
      "880/880 [==============================] - 0s 456us/step - loss: 0.3809 - acc: 0.8259 - val_loss: 0.6888 - val_acc: 0.7354\n",
      "Epoch 124/300\n",
      "880/880 [==============================] - 0s 456us/step - loss: 0.3725 - acc: 0.8372 - val_loss: 0.7001 - val_acc: 0.7335\n",
      "Epoch 125/300\n",
      "880/880 [==============================] - 0s 456us/step - loss: 0.3801 - acc: 0.8293 - val_loss: 0.6560 - val_acc: 0.7388\n",
      "Epoch 126/300\n",
      "880/880 [==============================] - 0s 460us/step - loss: 0.3794 - acc: 0.8261 - val_loss: 0.6502 - val_acc: 0.7381\n",
      "Epoch 127/300\n",
      "880/880 [==============================] - 0s 452us/step - loss: 0.3888 - acc: 0.8261 - val_loss: 0.7785 - val_acc: 0.7348\n",
      "Epoch 128/300\n",
      "880/880 [==============================] - 0s 461us/step - loss: 0.3821 - acc: 0.8301 - val_loss: 0.6641 - val_acc: 0.7328\n",
      "Epoch 129/300\n",
      "880/880 [==============================] - 0s 472us/step - loss: 0.3750 - acc: 0.8312 - val_loss: 0.6637 - val_acc: 0.7321\n",
      "Epoch 130/300\n",
      "880/880 [==============================] - 0s 449us/step - loss: 0.3780 - acc: 0.8295 - val_loss: 0.7055 - val_acc: 0.7335\n",
      "Epoch 131/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.3607 - acc: 0.8403 - val_loss: 0.6776 - val_acc: 0.7381\n",
      "Epoch 132/300\n",
      "880/880 [==============================] - 0s 449us/step - loss: 0.3697 - acc: 0.8318 - val_loss: 0.6675 - val_acc: 0.7328\n",
      "Epoch 133/300\n",
      "880/880 [==============================] - 0s 451us/step - loss: 0.3762 - acc: 0.8375 - val_loss: 0.7013 - val_acc: 0.7374\n",
      "Epoch 134/300\n",
      "880/880 [==============================] - 0s 462us/step - loss: 0.3687 - acc: 0.8364 - val_loss: 0.6665 - val_acc: 0.7381\n",
      "Epoch 135/300\n",
      "880/880 [==============================] - 0s 465us/step - loss: 0.3750 - acc: 0.8298 - val_loss: 0.6999 - val_acc: 0.7381\n",
      "Epoch 136/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.3622 - acc: 0.8386 - val_loss: 0.6663 - val_acc: 0.7381\n",
      "Epoch 137/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.3653 - acc: 0.8426 - val_loss: 0.6751 - val_acc: 0.7394\n",
      "Epoch 138/300\n",
      "880/880 [==============================] - 0s 462us/step - loss: 0.3570 - acc: 0.8420 - val_loss: 0.7189 - val_acc: 0.7328\n",
      "Epoch 139/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.3617 - acc: 0.8381 - val_loss: 0.6789 - val_acc: 0.7414\n",
      "Epoch 140/300\n",
      "880/880 [==============================] - 0s 460us/step - loss: 0.3551 - acc: 0.8480 - val_loss: 0.6572 - val_acc: 0.7388\n",
      "Epoch 141/300\n",
      "880/880 [==============================] - 0s 459us/step - loss: 0.3700 - acc: 0.8358 - val_loss: 0.6875 - val_acc: 0.7394\n",
      "Epoch 142/300\n",
      "880/880 [==============================] - 0s 470us/step - loss: 0.3710 - acc: 0.8318 - val_loss: 0.6785 - val_acc: 0.7427\n",
      "Epoch 143/300\n",
      "880/880 [==============================] - 0s 540us/step - loss: 0.3762 - acc: 0.8301 - val_loss: 0.7097 - val_acc: 0.7414\n",
      "Epoch 144/300\n",
      "880/880 [==============================] - 1s 587us/step - loss: 0.3551 - acc: 0.8449 - val_loss: 0.7732 - val_acc: 0.7421\n",
      "Epoch 145/300\n",
      "880/880 [==============================] - 1s 574us/step - loss: 0.3617 - acc: 0.8426 - val_loss: 0.7419 - val_acc: 0.7302\n",
      "Epoch 146/300\n",
      "880/880 [==============================] - 1s 588us/step - loss: 0.3595 - acc: 0.8364 - val_loss: 0.6970 - val_acc: 0.7381\n",
      "Epoch 147/300\n",
      "880/880 [==============================] - 0s 559us/step - loss: 0.3591 - acc: 0.8384 - val_loss: 0.7124 - val_acc: 0.7421\n",
      "Epoch 148/300\n",
      "880/880 [==============================] - 1s 573us/step - loss: 0.3683 - acc: 0.8352 - val_loss: 0.7447 - val_acc: 0.7348\n",
      "Epoch 149/300\n",
      "880/880 [==============================] - 0s 561us/step - loss: 0.3580 - acc: 0.8455 - val_loss: 0.6852 - val_acc: 0.7374\n",
      "Epoch 150/300\n",
      "880/880 [==============================] - 0s 509us/step - loss: 0.3543 - acc: 0.8457 - val_loss: 0.6862 - val_acc: 0.7401\n",
      "Epoch 151/300\n",
      "880/880 [==============================] - 0s 464us/step - loss: 0.3559 - acc: 0.8438 - val_loss: 0.7076 - val_acc: 0.7348\n",
      "Epoch 152/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.3498 - acc: 0.8438 - val_loss: 0.7213 - val_acc: 0.7394\n",
      "Epoch 153/300\n",
      "880/880 [==============================] - 0s 489us/step - loss: 0.3529 - acc: 0.8409 - val_loss: 0.7264 - val_acc: 0.7328\n",
      "Epoch 154/300\n",
      "880/880 [==============================] - 0s 473us/step - loss: 0.3509 - acc: 0.8435 - val_loss: 0.7249 - val_acc: 0.7374\n",
      "Epoch 155/300\n",
      "880/880 [==============================] - 0s 465us/step - loss: 0.3449 - acc: 0.8449 - val_loss: 0.7058 - val_acc: 0.7434\n",
      "Epoch 156/300\n",
      "880/880 [==============================] - 0s 468us/step - loss: 0.3429 - acc: 0.8494 - val_loss: 0.7247 - val_acc: 0.7321\n",
      "Epoch 157/300\n",
      "880/880 [==============================] - 0s 477us/step - loss: 0.3504 - acc: 0.8443 - val_loss: 0.6987 - val_acc: 0.7440\n",
      "Epoch 158/300\n",
      "880/880 [==============================] - 0s 481us/step - loss: 0.3404 - acc: 0.8483 - val_loss: 0.6899 - val_acc: 0.7440\n",
      "Epoch 159/300\n",
      "880/880 [==============================] - 0s 463us/step - loss: 0.3334 - acc: 0.8531 - val_loss: 0.6925 - val_acc: 0.7407\n",
      "Epoch 160/300\n",
      "880/880 [==============================] - 0s 475us/step - loss: 0.3425 - acc: 0.8486 - val_loss: 0.6789 - val_acc: 0.7474\n",
      "Epoch 161/300\n",
      "880/880 [==============================] - 0s 484us/step - loss: 0.3421 - acc: 0.8503 - val_loss: 0.7308 - val_acc: 0.7381\n",
      "Epoch 162/300\n",
      "880/880 [==============================] - 0s 469us/step - loss: 0.3309 - acc: 0.8537 - val_loss: 0.6987 - val_acc: 0.7447\n",
      "Epoch 163/300\n",
      "880/880 [==============================] - 0s 471us/step - loss: 0.3377 - acc: 0.8506 - val_loss: 0.7397 - val_acc: 0.7381\n",
      "Epoch 164/300\n",
      "880/880 [==============================] - 0s 475us/step - loss: 0.3268 - acc: 0.8537 - val_loss: 0.7029 - val_acc: 0.7388\n",
      "Epoch 165/300\n",
      "880/880 [==============================] - 0s 488us/step - loss: 0.3341 - acc: 0.8531 - val_loss: 0.7365 - val_acc: 0.7361\n",
      "Epoch 166/300\n",
      "880/880 [==============================] - 0s 568us/step - loss: 0.3263 - acc: 0.8588 - val_loss: 0.7223 - val_acc: 0.7434\n",
      "Epoch 167/300\n",
      "880/880 [==============================] - 1s 633us/step - loss: 0.3292 - acc: 0.8528 - val_loss: 0.7248 - val_acc: 0.7361\n",
      "Epoch 168/300\n",
      "880/880 [==============================] - 1s 640us/step - loss: 0.3264 - acc: 0.8565 - val_loss: 0.7551 - val_acc: 0.7315\n",
      "Epoch 169/300\n",
      "880/880 [==============================] - 1s 655us/step - loss: 0.3185 - acc: 0.8591 - val_loss: 0.7536 - val_acc: 0.7341\n",
      "Epoch 170/300\n",
      "880/880 [==============================] - 1s 638us/step - loss: 0.3191 - acc: 0.8582 - val_loss: 0.7388 - val_acc: 0.7374\n",
      "Epoch 171/300\n",
      "880/880 [==============================] - 1s 632us/step - loss: 0.3182 - acc: 0.8605 - val_loss: 0.7681 - val_acc: 0.7269\n",
      "Epoch 172/300\n",
      "880/880 [==============================] - 0s 565us/step - loss: 0.3311 - acc: 0.8563 - val_loss: 0.6962 - val_acc: 0.7447\n",
      "Epoch 173/300\n",
      "880/880 [==============================] - 0s 465us/step - loss: 0.3188 - acc: 0.8597 - val_loss: 0.8138 - val_acc: 0.7255\n",
      "Epoch 174/300\n",
      "880/880 [==============================] - 0s 464us/step - loss: 0.3371 - acc: 0.8554 - val_loss: 0.7222 - val_acc: 0.7381\n",
      "Epoch 175/300\n",
      "880/880 [==============================] - 0s 458us/step - loss: 0.3284 - acc: 0.8557 - val_loss: 0.7237 - val_acc: 0.7434\n",
      "Epoch 176/300\n",
      "880/880 [==============================] - 0s 481us/step - loss: 0.3221 - acc: 0.8554 - val_loss: 0.7102 - val_acc: 0.7440\n",
      "Epoch 177/300\n",
      "880/880 [==============================] - 0s 461us/step - loss: 0.3399 - acc: 0.8509 - val_loss: 0.7324 - val_acc: 0.7328\n",
      "Epoch 178/300\n",
      "880/880 [==============================] - 0s 467us/step - loss: 0.3288 - acc: 0.8548 - val_loss: 0.7690 - val_acc: 0.7308\n",
      "Epoch 179/300\n",
      "880/880 [==============================] - 0s 466us/step - loss: 0.3216 - acc: 0.8560 - val_loss: 0.7208 - val_acc: 0.7440\n",
      "Epoch 180/300\n",
      "880/880 [==============================] - 0s 517us/step - loss: 0.3356 - acc: 0.8489 - val_loss: 0.7490 - val_acc: 0.7440\n",
      "Epoch 181/300\n",
      "880/880 [==============================] - 1s 584us/step - loss: 0.3400 - acc: 0.8503 - val_loss: 0.7569 - val_acc: 0.7328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/300\n",
      "880/880 [==============================] - 0s 562us/step - loss: 0.3288 - acc: 0.8560 - val_loss: 0.7509 - val_acc: 0.7381\n",
      "Epoch 183/300\n",
      "880/880 [==============================] - 1s 571us/step - loss: 0.3215 - acc: 0.8597 - val_loss: 0.7385 - val_acc: 0.7262\n",
      "Epoch 184/300\n",
      "880/880 [==============================] - 1s 597us/step - loss: 0.3098 - acc: 0.8648 - val_loss: 0.7589 - val_acc: 0.7361\n",
      "Epoch 185/300\n",
      "880/880 [==============================] - 1s 587us/step - loss: 0.3203 - acc: 0.8560 - val_loss: 0.7458 - val_acc: 0.7354\n",
      "Epoch 186/300\n",
      "880/880 [==============================] - 1s 650us/step - loss: 0.3150 - acc: 0.8585 - val_loss: 0.8255 - val_acc: 0.7196\n",
      "Epoch 187/300\n",
      "880/880 [==============================] - 0s 567us/step - loss: 0.3095 - acc: 0.8628 - val_loss: 0.7476 - val_acc: 0.7361\n",
      "Epoch 188/300\n",
      "880/880 [==============================] - 0s 513us/step - loss: 0.3089 - acc: 0.8639 - val_loss: 0.7567 - val_acc: 0.7401\n",
      "Epoch 189/300\n",
      "880/880 [==============================] - 0s 514us/step - loss: 0.3139 - acc: 0.8665 - val_loss: 0.7323 - val_acc: 0.7368\n",
      "Epoch 190/300\n",
      "880/880 [==============================] - 0s 528us/step - loss: 0.3107 - acc: 0.8642 - val_loss: 0.7449 - val_acc: 0.7335\n",
      "Epoch 191/300\n",
      "880/880 [==============================] - 0s 512us/step - loss: 0.3092 - acc: 0.8636 - val_loss: 0.7626 - val_acc: 0.7401\n",
      "Epoch 192/300\n",
      "880/880 [==============================] - 0s 510us/step - loss: 0.3014 - acc: 0.8670 - val_loss: 0.7649 - val_acc: 0.7394\n",
      "Epoch 193/300\n",
      "880/880 [==============================] - 0s 513us/step - loss: 0.3087 - acc: 0.8659 - val_loss: 0.8518 - val_acc: 0.7242\n",
      "Epoch 194/300\n",
      "880/880 [==============================] - 0s 502us/step - loss: 0.3072 - acc: 0.8648 - val_loss: 0.7602 - val_acc: 0.7427\n",
      "Epoch 195/300\n",
      "880/880 [==============================] - 0s 501us/step - loss: 0.3028 - acc: 0.8673 - val_loss: 0.8717 - val_acc: 0.7136\n",
      "Epoch 196/300\n",
      "880/880 [==============================] - 0s 485us/step - loss: 0.3279 - acc: 0.8557 - val_loss: 0.8099 - val_acc: 0.7216\n",
      "Epoch 197/300\n",
      "880/880 [==============================] - 0s 514us/step - loss: 0.3118 - acc: 0.8611 - val_loss: 0.7699 - val_acc: 0.7361\n",
      "Epoch 198/300\n",
      "880/880 [==============================] - 0s 518us/step - loss: 0.3014 - acc: 0.8682 - val_loss: 0.8142 - val_acc: 0.7216\n",
      "Epoch 199/300\n",
      "880/880 [==============================] - 0s 530us/step - loss: 0.3041 - acc: 0.8676 - val_loss: 0.8342 - val_acc: 0.7255\n",
      "Epoch 200/300\n",
      "880/880 [==============================] - 0s 485us/step - loss: 0.3142 - acc: 0.8628 - val_loss: 0.7361 - val_acc: 0.7434\n",
      "Epoch 201/300\n",
      "880/880 [==============================] - 0s 482us/step - loss: 0.3057 - acc: 0.8622 - val_loss: 0.7552 - val_acc: 0.7374\n",
      "Epoch 202/300\n",
      "880/880 [==============================] - 1s 658us/step - loss: 0.3259 - acc: 0.8526 - val_loss: 0.7930 - val_acc: 0.7202\n",
      "Epoch 203/300\n",
      "880/880 [==============================] - 1s 655us/step - loss: 0.3059 - acc: 0.8668 - val_loss: 0.7947 - val_acc: 0.7335\n",
      "Epoch 204/300\n",
      "880/880 [==============================] - 1s 745us/step - loss: 0.3038 - acc: 0.8588 - val_loss: 0.7538 - val_acc: 0.7427\n",
      "Epoch 205/300\n",
      "880/880 [==============================] - 1s 736us/step - loss: 0.2995 - acc: 0.8713 - val_loss: 0.7688 - val_acc: 0.7321\n",
      "Epoch 206/300\n",
      "880/880 [==============================] - 1s 689us/step - loss: 0.3156 - acc: 0.8591 - val_loss: 0.8227 - val_acc: 0.7143\n",
      "Epoch 207/300\n",
      "880/880 [==============================] - 1s 714us/step - loss: 0.3078 - acc: 0.8648 - val_loss: 0.7891 - val_acc: 0.7315\n",
      "Epoch 208/300\n",
      "880/880 [==============================] - 0s 540us/step - loss: 0.2959 - acc: 0.8688 - val_loss: 0.8069 - val_acc: 0.7275\n",
      "Epoch 209/300\n",
      "880/880 [==============================] - 0s 520us/step - loss: 0.2824 - acc: 0.8727 - val_loss: 0.7656 - val_acc: 0.7388\n",
      "Epoch 210/300\n",
      "880/880 [==============================] - 0s 537us/step - loss: 0.2872 - acc: 0.8730 - val_loss: 0.8345 - val_acc: 0.7209\n",
      "Epoch 211/300\n",
      "880/880 [==============================] - 0s 552us/step - loss: 0.2971 - acc: 0.8716 - val_loss: 0.7790 - val_acc: 0.7275\n",
      "Epoch 212/300\n",
      "880/880 [==============================] - 0s 536us/step - loss: 0.3033 - acc: 0.8702 - val_loss: 0.7699 - val_acc: 0.7328\n",
      "Epoch 213/300\n",
      "880/880 [==============================] - 0s 535us/step - loss: 0.3114 - acc: 0.8653 - val_loss: 0.8182 - val_acc: 0.7235\n",
      "Epoch 214/300\n",
      "880/880 [==============================] - 0s 486us/step - loss: 0.3174 - acc: 0.8605 - val_loss: 0.8511 - val_acc: 0.7288\n",
      "Epoch 215/300\n",
      "880/880 [==============================] - 1s 574us/step - loss: 0.3042 - acc: 0.8668 - val_loss: 0.8251 - val_acc: 0.7255\n",
      "Epoch 216/300\n",
      "880/880 [==============================] - 1s 594us/step - loss: 0.2887 - acc: 0.8724 - val_loss: 0.8394 - val_acc: 0.7269\n",
      "Epoch 217/300\n",
      "880/880 [==============================] - 1s 634us/step - loss: 0.2924 - acc: 0.8759 - val_loss: 0.8242 - val_acc: 0.7216\n",
      "Epoch 218/300\n",
      "880/880 [==============================] - 1s 605us/step - loss: 0.2898 - acc: 0.8756 - val_loss: 0.8225 - val_acc: 0.7255\n",
      "Epoch 219/300\n",
      "880/880 [==============================] - 1s 601us/step - loss: 0.2802 - acc: 0.8741 - val_loss: 0.8054 - val_acc: 0.7308\n",
      "Epoch 220/300\n",
      "880/880 [==============================] - 1s 624us/step - loss: 0.2952 - acc: 0.8648 - val_loss: 0.8045 - val_acc: 0.7288\n",
      "Epoch 221/300\n",
      "880/880 [==============================] - 1s 809us/step - loss: 0.2892 - acc: 0.8722 - val_loss: 0.8902 - val_acc: 0.7149\n",
      "Epoch 222/300\n",
      "880/880 [==============================] - 0s 524us/step - loss: 0.2758 - acc: 0.8756 - val_loss: 0.8564 - val_acc: 0.7156\n",
      "Epoch 223/300\n",
      "880/880 [==============================] - 0s 520us/step - loss: 0.2924 - acc: 0.8707 - val_loss: 0.7763 - val_acc: 0.7302\n",
      "Epoch 224/300\n",
      "880/880 [==============================] - 0s 535us/step - loss: 0.3077 - acc: 0.8639 - val_loss: 0.8252 - val_acc: 0.7235\n",
      "Epoch 225/300\n",
      "880/880 [==============================] - 0s 566us/step - loss: 0.3008 - acc: 0.8659 - val_loss: 0.8688 - val_acc: 0.7242\n",
      "Epoch 226/300\n",
      "880/880 [==============================] - 1s 584us/step - loss: 0.3062 - acc: 0.8636 - val_loss: 0.8216 - val_acc: 0.7235\n",
      "Epoch 227/300\n",
      "880/880 [==============================] - 0s 504us/step - loss: 0.2955 - acc: 0.8710 - val_loss: 0.8311 - val_acc: 0.7295\n",
      "Epoch 228/300\n",
      "880/880 [==============================] - 1s 599us/step - loss: 0.2878 - acc: 0.8707 - val_loss: 0.9242 - val_acc: 0.7116\n",
      "Epoch 229/300\n",
      "880/880 [==============================] - 1s 622us/step - loss: 0.2875 - acc: 0.8705 - val_loss: 0.8989 - val_acc: 0.7136\n",
      "Epoch 230/300\n",
      "880/880 [==============================] - 1s 652us/step - loss: 0.3107 - acc: 0.8605 - val_loss: 0.9247 - val_acc: 0.7176\n",
      "Epoch 231/300\n",
      "880/880 [==============================] - 0s 566us/step - loss: 0.2785 - acc: 0.8824 - val_loss: 0.8256 - val_acc: 0.7242\n",
      "Epoch 232/300\n",
      "880/880 [==============================] - 0s 564us/step - loss: 0.2767 - acc: 0.8807 - val_loss: 0.9481 - val_acc: 0.7044\n",
      "Epoch 233/300\n",
      "880/880 [==============================] - 0s 560us/step - loss: 0.2658 - acc: 0.8810 - val_loss: 0.8981 - val_acc: 0.7189\n",
      "Epoch 234/300\n",
      "880/880 [==============================] - 0s 502us/step - loss: 0.2673 - acc: 0.8798 - val_loss: 0.8424 - val_acc: 0.7262\n",
      "Epoch 235/300\n",
      "880/880 [==============================] - 0s 454us/step - loss: 0.2698 - acc: 0.8812 - val_loss: 0.8789 - val_acc: 0.7176\n",
      "Epoch 236/300\n",
      "880/880 [==============================] - 0s 427us/step - loss: 0.2747 - acc: 0.8807 - val_loss: 0.9273 - val_acc: 0.7202\n",
      "Epoch 237/300\n",
      "880/880 [==============================] - 0s 449us/step - loss: 0.2716 - acc: 0.8824 - val_loss: 0.8500 - val_acc: 0.7275\n",
      "Epoch 238/300\n",
      "880/880 [==============================] - 0s 457us/step - loss: 0.2589 - acc: 0.8861 - val_loss: 0.8712 - val_acc: 0.7149\n",
      "Epoch 239/300\n",
      "880/880 [==============================] - 0s 436us/step - loss: 0.2676 - acc: 0.8830 - val_loss: 0.8192 - val_acc: 0.7269\n",
      "Epoch 240/300\n",
      "880/880 [==============================] - 0s 442us/step - loss: 0.2731 - acc: 0.8776 - val_loss: 0.8822 - val_acc: 0.7315\n",
      "Epoch 241/300\n",
      "880/880 [==============================] - 0s 436us/step - loss: 0.2678 - acc: 0.8841 - val_loss: 0.9017 - val_acc: 0.7235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/300\n",
      "880/880 [==============================] - 0s 471us/step - loss: 0.2732 - acc: 0.8798 - val_loss: 1.0429 - val_acc: 0.7136\n",
      "Epoch 243/300\n",
      "880/880 [==============================] - 0s 547us/step - loss: 0.2701 - acc: 0.8827 - val_loss: 0.8048 - val_acc: 0.7295\n",
      "Epoch 244/300\n",
      "880/880 [==============================] - 0s 561us/step - loss: 0.2809 - acc: 0.8781 - val_loss: 0.8409 - val_acc: 0.7282\n",
      "Epoch 245/300\n",
      "880/880 [==============================] - 0s 539us/step - loss: 0.2628 - acc: 0.8832 - val_loss: 0.9443 - val_acc: 0.7083\n",
      "Epoch 246/300\n",
      "880/880 [==============================] - 1s 569us/step - loss: 0.2684 - acc: 0.8812 - val_loss: 0.9060 - val_acc: 0.7229\n",
      "Epoch 247/300\n",
      "880/880 [==============================] - 1s 568us/step - loss: 0.2816 - acc: 0.8756 - val_loss: 0.9382 - val_acc: 0.7196\n",
      "Epoch 248/300\n",
      "880/880 [==============================] - 0s 562us/step - loss: 0.2739 - acc: 0.8827 - val_loss: 0.9442 - val_acc: 0.7017\n",
      "Epoch 249/300\n",
      "880/880 [==============================] - 0s 537us/step - loss: 0.2723 - acc: 0.8827 - val_loss: 0.9166 - val_acc: 0.7209\n",
      "Epoch 250/300\n",
      "880/880 [==============================] - 0s 490us/step - loss: 0.2649 - acc: 0.8818 - val_loss: 0.9045 - val_acc: 0.7116\n",
      "Epoch 251/300\n",
      "880/880 [==============================] - 0s 548us/step - loss: 0.2588 - acc: 0.8844 - val_loss: 0.8609 - val_acc: 0.7202\n",
      "Epoch 252/300\n",
      "880/880 [==============================] - 0s 482us/step - loss: 0.2662 - acc: 0.8855 - val_loss: 0.8799 - val_acc: 0.7189\n",
      "Epoch 253/300\n",
      "880/880 [==============================] - 0s 477us/step - loss: 0.2534 - acc: 0.8892 - val_loss: 0.9110 - val_acc: 0.7110\n",
      "Epoch 254/300\n",
      "880/880 [==============================] - 0s 449us/step - loss: 0.2724 - acc: 0.8798 - val_loss: 0.8869 - val_acc: 0.7242\n",
      "Epoch 255/300\n",
      "880/880 [==============================] - 0s 453us/step - loss: 0.2597 - acc: 0.8852 - val_loss: 0.9699 - val_acc: 0.7143\n",
      "Epoch 256/300\n",
      "880/880 [==============================] - 0s 436us/step - loss: 0.2617 - acc: 0.8841 - val_loss: 0.9659 - val_acc: 0.7090\n",
      "Epoch 257/300\n",
      "880/880 [==============================] - 0s 477us/step - loss: 0.2539 - acc: 0.8901 - val_loss: 1.0107 - val_acc: 0.7090\n",
      "Epoch 258/300\n",
      "880/880 [==============================] - 0s 546us/step - loss: 0.2648 - acc: 0.8901 - val_loss: 0.9420 - val_acc: 0.7163\n",
      "Epoch 259/300\n",
      "880/880 [==============================] - 0s 562us/step - loss: 0.2592 - acc: 0.8872 - val_loss: 0.8919 - val_acc: 0.7229\n",
      "Epoch 260/300\n",
      "880/880 [==============================] - 0s 542us/step - loss: 0.2611 - acc: 0.8832 - val_loss: 0.9743 - val_acc: 0.7169\n",
      "Epoch 261/300\n",
      "880/880 [==============================] - 0s 545us/step - loss: 0.2608 - acc: 0.8841 - val_loss: 1.0859 - val_acc: 0.7044\n",
      "Epoch 262/300\n",
      "880/880 [==============================] - 1s 625us/step - loss: 0.2793 - acc: 0.8790 - val_loss: 0.9569 - val_acc: 0.7183\n",
      "Epoch 263/300\n",
      "880/880 [==============================] - 0s 435us/step - loss: 0.2589 - acc: 0.8881 - val_loss: 0.9603 - val_acc: 0.7176\n",
      "Epoch 264/300\n",
      "880/880 [==============================] - 0s 423us/step - loss: 0.2500 - acc: 0.8918 - val_loss: 0.9010 - val_acc: 0.7202\n",
      "Epoch 265/300\n",
      "880/880 [==============================] - 0s 476us/step - loss: 0.2750 - acc: 0.8821 - val_loss: 0.9016 - val_acc: 0.7202\n",
      "Epoch 266/300\n",
      "880/880 [==============================] - 0s 521us/step - loss: 0.2514 - acc: 0.8872 - val_loss: 0.9858 - val_acc: 0.7143\n",
      "Epoch 267/300\n",
      "880/880 [==============================] - 0s 555us/step - loss: 0.2489 - acc: 0.8923 - val_loss: 0.9573 - val_acc: 0.7024\n",
      "Epoch 268/300\n",
      "880/880 [==============================] - 1s 662us/step - loss: 0.2421 - acc: 0.8918 - val_loss: 0.9492 - val_acc: 0.7163\n",
      "Epoch 269/300\n",
      "880/880 [==============================] - 1s 603us/step - loss: 0.2482 - acc: 0.8886 - val_loss: 0.9776 - val_acc: 0.7103\n",
      "Epoch 270/300\n",
      "880/880 [==============================] - 1s 642us/step - loss: 0.2403 - acc: 0.8963 - val_loss: 0.9842 - val_acc: 0.7209\n",
      "Epoch 271/300\n",
      "880/880 [==============================] - 1s 644us/step - loss: 0.2630 - acc: 0.8869 - val_loss: 0.8672 - val_acc: 0.7202\n",
      "Epoch 272/300\n",
      "880/880 [==============================] - 1s 696us/step - loss: 0.2677 - acc: 0.8855 - val_loss: 0.9987 - val_acc: 0.7136\n",
      "Epoch 273/300\n",
      "880/880 [==============================] - 1s 615us/step - loss: 0.2605 - acc: 0.8881 - val_loss: 0.9092 - val_acc: 0.7216\n",
      "Epoch 274/300\n",
      "880/880 [==============================] - 1s 653us/step - loss: 0.2730 - acc: 0.8824 - val_loss: 1.0185 - val_acc: 0.7249\n",
      "Epoch 275/300\n",
      "880/880 [==============================] - 1s 599us/step - loss: 0.2587 - acc: 0.8878 - val_loss: 1.0211 - val_acc: 0.7176\n",
      "Epoch 276/300\n",
      "880/880 [==============================] - 1s 607us/step - loss: 0.2512 - acc: 0.8886 - val_loss: 0.9469 - val_acc: 0.7242\n",
      "Epoch 277/300\n",
      "880/880 [==============================] - 0s 527us/step - loss: 0.2487 - acc: 0.8923 - val_loss: 0.9239 - val_acc: 0.7209\n",
      "Epoch 278/300\n",
      "880/880 [==============================] - 0s 520us/step - loss: 0.2406 - acc: 0.8960 - val_loss: 0.9188 - val_acc: 0.7209\n",
      "Epoch 279/300\n",
      "880/880 [==============================] - 0s 523us/step - loss: 0.2411 - acc: 0.8929 - val_loss: 0.9433 - val_acc: 0.7130\n",
      "Epoch 280/300\n",
      "880/880 [==============================] - 0s 511us/step - loss: 0.2397 - acc: 0.8943 - val_loss: 0.9892 - val_acc: 0.7110\n",
      "Epoch 281/300\n",
      "880/880 [==============================] - 0s 517us/step - loss: 0.2667 - acc: 0.8858 - val_loss: 0.8863 - val_acc: 0.7249\n",
      "Epoch 282/300\n",
      "880/880 [==============================] - 1s 604us/step - loss: 0.2580 - acc: 0.8892 - val_loss: 0.8735 - val_acc: 0.7288\n",
      "Epoch 283/300\n",
      "880/880 [==============================] - 1s 642us/step - loss: 0.2583 - acc: 0.8875 - val_loss: 0.9291 - val_acc: 0.7176\n",
      "Epoch 284/300\n",
      "880/880 [==============================] - 1s 651us/step - loss: 0.2488 - acc: 0.8881 - val_loss: 0.9173 - val_acc: 0.7176\n",
      "Epoch 285/300\n",
      "880/880 [==============================] - 1s 628us/step - loss: 0.2393 - acc: 0.8923 - val_loss: 0.9538 - val_acc: 0.7130\n",
      "Epoch 286/300\n",
      "880/880 [==============================] - 1s 706us/step - loss: 0.2548 - acc: 0.8849 - val_loss: 0.9555 - val_acc: 0.7183\n",
      "Epoch 287/300\n",
      "880/880 [==============================] - 1s 672us/step - loss: 0.2693 - acc: 0.8812 - val_loss: 0.9902 - val_acc: 0.7116\n",
      "Epoch 288/300\n",
      "880/880 [==============================] - 1s 613us/step - loss: 0.2504 - acc: 0.8920 - val_loss: 0.9539 - val_acc: 0.7169\n",
      "Epoch 289/300\n",
      "880/880 [==============================] - 0s 530us/step - loss: 0.2413 - acc: 0.8943 - val_loss: 1.0189 - val_acc: 0.7103\n",
      "Epoch 290/300\n",
      "880/880 [==============================] - 0s 517us/step - loss: 0.2628 - acc: 0.8815 - val_loss: 0.9945 - val_acc: 0.7097\n",
      "Epoch 291/300\n",
      "880/880 [==============================] - 0s 539us/step - loss: 0.2465 - acc: 0.8918 - val_loss: 1.0127 - val_acc: 0.7037\n",
      "Epoch 292/300\n",
      "880/880 [==============================] - 0s 566us/step - loss: 0.2378 - acc: 0.8966 - val_loss: 0.9724 - val_acc: 0.7156\n",
      "Epoch 293/300\n",
      "880/880 [==============================] - 0s 524us/step - loss: 0.2393 - acc: 0.8952 - val_loss: 0.9427 - val_acc: 0.7209\n",
      "Epoch 294/300\n",
      "880/880 [==============================] - 0s 518us/step - loss: 0.2321 - acc: 0.9003 - val_loss: 1.0370 - val_acc: 0.7070\n",
      "Epoch 295/300\n",
      "880/880 [==============================] - 0s 520us/step - loss: 0.2441 - acc: 0.8955 - val_loss: 0.9881 - val_acc: 0.7143\n",
      "Epoch 296/300\n",
      "880/880 [==============================] - 1s 636us/step - loss: 0.2428 - acc: 0.8946 - val_loss: 1.0772 - val_acc: 0.7097\n",
      "Epoch 297/300\n",
      "880/880 [==============================] - 1s 720us/step - loss: 0.2353 - acc: 0.8980 - val_loss: 1.1184 - val_acc: 0.6984\n",
      "Epoch 298/300\n",
      "880/880 [==============================] - 1s 655us/step - loss: 0.2433 - acc: 0.8918 - val_loss: 0.9979 - val_acc: 0.7090\n",
      "Epoch 299/300\n",
      "880/880 [==============================] - 1s 640us/step - loss: 0.2455 - acc: 0.8909 - val_loss: 0.9494 - val_acc: 0.7189\n",
      "Epoch 300/300\n",
      "880/880 [==============================] - 1s 659us/step - loss: 0.2397 - acc: 0.8952 - val_loss: 1.1275 - val_acc: 0.7050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f13476b3a58>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "model.fit(X_train, y_train_hot, epochs=config.epochs, validation_data=(X_test, y_test_hot), callbacks=[WandbCallback(data_type=\"image\", labels=label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
